{
  "id": "high-performance-computing-lab",
  "title": "AWS HPC Services to Azure HPC: High Performance Computing Migration and Optimization",
  "difficulty": "expert",
  "estimated_time": "300 minutes",
  "aws_prerequisite": "Advanced experience with AWS Batch, ParallelCluster, EC2 placement groups, enhanced networking, Lustre file systems, and HPC workload optimization",
  "azure_target": "Master Azure Batch, HPC Cache, Azure CycleCloud, high-performance networking, and comprehensive HPC architecture patterns",
  "learning_objectives": [
    "Design sophisticated HPC architectures equivalent to AWS ParallelCluster",
    "Implement advanced batch processing with Azure Batch and CycleCloud",
    "Configure high-performance storage with HPC Cache and parallel file systems",
    "Set up enhanced networking for low-latency HPC workloads",
    "Master GPU computing and AI/ML workload optimization",
    "Implement HPC cost optimization with spot instances and auto-scaling",
    "Configure HPC monitoring, performance analysis, and optimization",
    "Design hybrid HPC architectures with on-premises integration"
  ],
  "aws_context": {
    "hpc_services_mapping": {
      "aws_parallelcluster": "Azure CycleCloud",
      "aws_batch": "Azure Batch",
      "amazon_fsx_lustre": "Azure HPC Cache + Parallel File Systems",
      "placement_groups": "Proximity Placement Groups + Availability Sets",
      "enhanced_networking": "Accelerated Networking + InfiniBand",
      "elastic_fabric_adapter": "InfiniBand networking in Azure",
      "ec2_hpc_instances": "HBv3, HCv1, NCv3, NDv2 series VMs",
      "aws_thinkbox_deadline": "Azure Render Farm Manager",
      "cloudwatch_monitoring": "Azure Monitor + HPC diagnostics"
    },
    "hpc_workload_patterns": {
      "monte_carlo_simulations": "Embarrassingly parallel workloads",
      "cfd_modeling": "Tightly coupled MPI workloads", 
      "genomic_analysis": "Data-intensive parallel processing",
      "financial_modeling": "Low-latency compute requirements",
      "rendering_pipelines": "GPU-intensive batch processing",
      "machine_learning": "Distributed training and inference"
    },
    "storage_patterns": {
      "scratch_storage": "High IOPS temporary storage for job data",
      "shared_storage": "Parallel file systems for multi-node access",
      "archive_storage": "Long-term data retention for results",
      "data_staging": "High-throughput data movement patterns"
    }
  },
  "sections": [
    {
      "title": "Advanced HPC Cluster Architecture Design",
      "content": "Design enterprise-grade HPC clusters with Azure CycleCloud equivalent to AWS ParallelCluster capabilities.",
      "cluster_design_patterns": {
        "hpc_cluster_topology": {
          "head_node": {
            "vm_size": "Standard_D8s_v3",
            "purpose": "Cluster management, job scheduling, user access",
            "redundancy": "High availability with paired head nodes",
            "storage": "Premium SSD for OS and cluster metadata"
          },
          "compute_nodes": [
            {
              "pool_name": "compute-cpu",
              "vm_size": "Standard_HBv3-120",
              "cpu_cores": 120,
              "memory_gb": 456,
              "network": "200 Gbps InfiniBand",
              "auto_scale": "0-100 nodes based on queue demand",
              "use_case": "Tightly coupled MPI workloads"
            },
            {
              "pool_name": "compute-gpu",
              "vm_size": "Standard_ND96asr_v4",
              "gpu_count": 8,
              "gpu_type": "A100 40GB",
              "cpu_cores": 96,
              "memory_gb": 900,
              "use_case": "AI/ML training and GPU computing"
            },
            {
              "pool_name": "compute-memory",
              "vm_size": "Standard_M128s",
              "cpu_cores": 128,
              "memory_gb": 2048,
              "network": "Accelerated networking",
              "use_case": "Memory-intensive workloads"
            }
          ],
          "storage_configuration": {
            "hpc_cache": "24 TB cache with 1.2 GB/s throughput",
            "parallel_file_system": "Lustre-compatible with 20 GB/s aggregate",
            "scratch_storage": "NVMe SSD local storage per compute node",
            "archive_storage": "Azure Blob Storage with lifecycle policies"
          }
        }
      },
      "code_examples": {
        "setup_hpc_environment": {
          "language": "bash",
          "title": "Setup Comprehensive HPC Environment",
          "code": "# Setup Azure HPC environment with CycleCloud and Batch\n\necho \"Setting up Azure HPC environment...\"\n\n# Create resource group for HPC workloads\naz group create \\\n  --name hpc-cluster-rg \\\n  --location eastus \\\n  --tags Environment=HPC Project=ScientificComputing CostCenter=HPC001\n\n# Create virtual network for HPC cluster\necho \"Creating HPC network infrastructure...\"\naz network vnet create \\\n  --resource-group hpc-cluster-rg \\\n  --name hpc-cluster-vnet \\\n  --address-prefixes 10.20.0.0/16 \\\n  --subnet-name hpc-compute-subnet \\\n  --subnet-prefixes 10.20.1.0/24\n\n# Create additional subnets for different HPC components\naz network vnet subnet create \\\n  --resource-group hpc-cluster-rg \\\n  --vnet-name hpc-cluster-vnet \\\n  --name hpc-storage-subnet \\\n  --address-prefixes 10.20.2.0/24\n\naz network vnet subnet create \\\n  --resource-group hpc-cluster-rg \\\n  --vnet-name hpc-cluster-vnet \\\n  --name hpc-management-subnet \\\n  --address-prefixes 10.20.3.0/24\n\n# Create Network Security Group for HPC traffic\necho \"Creating Network Security Group for HPC...\"\naz network nsg create \\\n  --resource-group hpc-cluster-rg \\\n  --name hpc-cluster-nsg\n\n# Add rules for HPC protocols\naz network nsg rule create \\\n  --resource-group hpc-cluster-rg \\\n  --nsg-name hpc-cluster-nsg \\\n  --name AllowSSH \\\n  --protocol tcp \\\n  --priority 1000 \\\n  --destination-port-range 22 \\\n  --access allow\n\naz network nsg rule create \\\n  --resource-group hpc-cluster-rg \\\n  --nsg-name hpc-cluster-nsg \\\n  --name AllowHPCScheduler \\\n  --protocol tcp \\\n  --priority 1100 \\\n  --destination-port-range 6817 \\\n  --access allow\n\naz network nsg rule create \\\n  --resource-group hpc-cluster-rg \\\n  --nsg-name hpc-cluster-nsg \\\n  --name AllowInfiniBand \\\n  --protocol tcp \\\n  --priority 1200 \\\n  --destination-port-range 4791 \\\n  --access allow\n\n# Associate NSG with subnets\naz network vnet subnet update \\\n  --resource-group hpc-cluster-rg \\\n  --vnet-name hpc-cluster-vnet \\\n  --name hpc-compute-subnet \\\n  --network-security-group hpc-cluster-nsg\n\n# Create storage account for HPC artifacts\necho \"Creating storage account for HPC...\"\nHPC_STORAGE_NAME=\"hpcstg$(openssl rand -hex 3)\"\naz storage account create \\\n  --resource-group hpc-cluster-rg \\\n  --name $HPC_STORAGE_NAME \\\n  --location eastus \\\n  --sku Premium_LRS \\\n  --kind FileStorage \\\n  --enable-large-file-share\n\n# Create file shares for HPC workloads\necho \"Creating Azure Files shares for HPC...\"\naz storage share create \\\n  --account-name $HPC_STORAGE_NAME \\\n  --name hpc-shared \\\n  --quota 5120 \\\n  --auth-mode login\n\naz storage share create \\\n  --account-name $HPC_STORAGE_NAME \\\n  --name hpc-scratch \\\n  --quota 10240 \\\n  --auth-mode login\n\naz storage share create \\\n  --account-name $HPC_STORAGE_NAME \\\n  --name hpc-apps \\\n  --quota 1024 \\\n  --auth-mode login\n\n# Create HPC Cache for high-performance storage\necho \"Creating HPC Cache...\"\nHPC_CACHE_NAME=\"hpc-cache-$(openssl rand -hex 3)\"\naz hpc-cache create \\\n  --resource-group hpc-cluster-rg \\\n  --name $HPC_CACHE_NAME \\\n  --location eastus \\\n  --cache-size-gb 3072 \\\n  --sku Standard_2G \\\n  --subnet $(az network vnet subnet show --resource-group hpc-cluster-rg --vnet-name hpc-cluster-vnet --name hpc-storage-subnet --query id -o tsv) \\\n  --tags Environment=HPC Purpose=HighPerformanceStorage\n\necho \"Waiting for HPC Cache to be created (this may take 15-20 minutes)...\"\n\n# Create Azure Batch account for job scheduling\necho \"Creating Azure Batch account...\"\nBATCH_ACCOUNT_NAME=\"hpcbatch$(openssl rand -hex 3)\"\naz batch account create \\\n  --resource-group hpc-cluster-rg \\\n  --name $BATCH_ACCOUNT_NAME \\\n  --location eastus \\\n  --storage-account $HPC_STORAGE_NAME\n\n# Create Key Vault for HPC secrets and certificates\necho \"Creating Key Vault for HPC certificates...\"\nHPC_KEYVAULT_NAME=\"hpc-kv-$(openssl rand -hex 3)\"\naz keyvault create \\\n  --resource-group hpc-cluster-rg \\\n  --name $HPC_KEYVAULT_NAME \\\n  --location eastus \\\n  --enable-soft-delete true\n\n# Create Log Analytics workspace for HPC monitoring\necho \"Creating Log Analytics workspace for HPC monitoring...\"\naz monitor log-analytics workspace create \\\n  --resource-group hpc-cluster-rg \\\n  --workspace-name hpc-monitoring-workspace \\\n  --location eastus \\\n  --sku pergb2018\n\nHPC_WORKSPACE_ID=$(az monitor log-analytics workspace show \\\n  --resource-group hpc-cluster-rg \\\n  --workspace-name hpc-monitoring-workspace \\\n  --query customerId -o tsv)\n\necho \"HPC environment setup completed.\"\necho \"Storage Account: $HPC_STORAGE_NAME\"\necho \"HPC Cache: $HPC_CACHE_NAME\"\necho \"Batch Account: $BATCH_ACCOUNT_NAME\"\necho \"Key Vault: $HPC_KEYVAULT_NAME\""
        },
        "create_hpc_compute_pools": {
          "language": "bash",\n          \"title\": \"Create Specialized HPC Compute Pools\",\n          \"code\": \"# Create specialized compute pools for different HPC workloads\\n\\necho \\\"Creating HPC compute pools...\\\"\\n\\n# Set Batch account context\\naz batch account set \\\\\\n  --name $BATCH_ACCOUNT_NAME \\\\\\n  --resource-group hpc-cluster-rg\\n\\n# Create CPU-intensive compute pool (HBv3 for MPI workloads)\\necho \\\"Creating CPU-intensive compute pool...\\\"\\ncat > hpc-cpu-pool-config.json << 'EOF'\\n{\\n  \\\"id\\\": \\\"hpc-cpu-pool\\\",\\n  \\\"displayName\\\": \\\"HPC CPU Pool - HBv3 instances\\\",\\n  \\\"vmSize\\\": \\\"Standard_HB120rs_v3\\\",\\n  \\\"virtualMachineConfiguration\\\": {\\n    \\\"imageReference\\\": {\\n      \\\"publisher\\\": \\\"microsoft-hpc\\\",\\n      \\\"offer\\\": \\\"hpc-pack-2019\\\",\\n      \\\"sku\\\": \\\"hpc-pack-2019-datacenter\\\",\\n      \\\"version\\\": \\\"latest\\\"\\n    },\\n    \\\"nodeAgentSkuId\\\": \\\"batch.node.windows amd64\\\"\\n  },\\n  \\\"targetDedicatedNodes\\\": 0,\\n  \\\"targetLowPriorityNodes\\\": 4,\\n  \\\"enableAutoScale\\\": true,\\n  \\\"autoScaleFormula\\\": \\\"// Auto-scale based on pending tasks and time of day\\\\n$samples = $ActiveTasks.GetSamplePercent(TimeInterval_Minute * 15);\\\\n$tasks = $samples < 70 ? max(0, $ActiveTasks.GetSample(1)) : max(0, avg($ActiveTasks.GetSample(TimeInterval_Minute * 15)));\\\\n$targetVMs = $tasks > 0 ? $tasks : 0;\\\\n$TargetDedicated = max(0, min(10, $targetVMs));\\\\n$TargetLowPriority = max(0, min(20, $tasks - $TargetDedicated));\\\",\\n  \\\"autoScaleEvaluationInterval\\\": \\\"PT5M\\\",\\n  \\\"enableInterNodeCommunication\\\": true,\\n  \\\"maxTasksPerNode\\\": 1,\\n  \\\"taskSlotsPerNode\\\": 1,\\n  \\\"taskSchedulingPolicy\\\": {\\n    \\\"nodeFillType\\\": \\\"Pack\\\"\\n  },\\n  \\\"userAccounts\\\": [\\n    {\\n      \\\"name\\\": \\\"hpcuser\\\",\\n      \\\"password\\\": \\\"HPC_User_P@ssw0rd!\\\",\\n      \\\"elevationLevel\\\": \\\"Admin\\\",\\n      \\\"linuxUserConfiguration\\\": {\\n        \\\"uid\\\": 1001,\\n        \\\"gid\\\": 1001\\n      }\\n    }\\n  ],\\n  \\\"mountConfiguration\\\": [\\n    {\\n      \\\"azureFileShareConfiguration\\\": {\\n        \\\"accountName\\\": \\\"HPC_STORAGE_NAME\\\",\\n        \\\"azureFileUrl\\\": \\\"https://HPC_STORAGE_NAME.file.core.windows.net/hpc-shared\\\",\\n        \\\"accountKey\\\": \\\"STORAGE_KEY\\\",\\n        \\\"relativeMountPath\\\": \\\"shared\\\",\\n        \\\"mountOptions\\\": \\\"-o vers=3.0,dir_mode=0777,file_mode=0777\\\"\\n      }\\n    }\\n  ]\\n}\\nEOF\\n\\n# Get storage account key\\nSTORAGE_KEY=$(az storage account keys list \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --account-name $HPC_STORAGE_NAME \\\\\\n  --query '[0].value' -o tsv)\\n\\nsed -i \\\"s/HPC_STORAGE_NAME/$HPC_STORAGE_NAME/g\\\" hpc-cpu-pool-config.json\\nsed -i \\\"s/STORAGE_KEY/$STORAGE_KEY/g\\\" hpc-cpu-pool-config.json\\n\\n# Create the CPU pool\\naz batch pool create \\\\\\n  --json-file hpc-cpu-pool-config.json\\n\\n# Create GPU compute pool (NDv2 for AI/ML workloads)\\necho \\\"Creating GPU compute pool...\\\"\\ncat > hpc-gpu-pool-config.json << 'EOF'\\n{\\n  \\\"id\\\": \\\"hpc-gpu-pool\\\",\\n  \\\"displayName\\\": \\\"HPC GPU Pool - NDv2 instances\\\",\\n  \\\"vmSize\\\": \\\"Standard_ND40rs_v2\\\",\\n  \\\"virtualMachineConfiguration\\\": {\\n    \\\"imageReference\\\": {\\n      \\\"publisher\\\": \\\"microsoft-dsvm\\\",\\n      \\\"offer\\\": \\\"ubuntu-hpc\\\",\\n      \\\"sku\\\": \\\"2004-gen2\\\",\\n      \\\"version\\\": \\\"latest\\\"\\n    },\\n    \\\"nodeAgentSkuId\\\": \\\"batch.node.ubuntu 20.04\\\"\\n  },\\n  \\\"targetDedicatedNodes\\\": 0,\\n  \\\"targetLowPriorityNodes\\\": 2,\\n  \\\"enableAutoScale\\\": true,\\n  \\\"autoScaleFormula\\\": \\\"// GPU-specific auto-scale formula\\\\n$samples = $ActiveTasks.GetSamplePercent(TimeInterval_Minute * 10);\\\\n$tasks = $samples < 70 ? max(0, $ActiveTasks.GetSample(1)) : max(0, avg($ActiveTasks.GetSample(TimeInterval_Minute * 10)));\\\\n$gpuTasks = $tasks; // Assume 1 task per GPU node\\\\n$TargetDedicated = max(0, min(2, $gpuTasks));\\\\n$TargetLowPriority = max(0, min(8, $tasks - $TargetDedicated));\\\",\\n  \\\"autoScaleEvaluationInterval\\\": \\\"PT5M\\\",\\n  \\\"enableInterNodeCommunication\\\": false,\\n  \\\"maxTasksPerNode\\\": 8,\\n  \\\"taskSlotsPerNode\\\": 8,\\n  \\\"startTask\\\": {\\n    \\\"commandLine\\\": \\\"/bin/bash -c 'nvidia-smi && python3 -c \\\\\\\"import torch; print(torch.cuda.is_available())\\\\\\\"'\\\",\\n    \\\"userIdentity\\\": {\\n      \\\"autoUser\\\": {\\n        \\\"scope\\\": \\\"Pool\\\",\\n        \\\"elevationLevel\\\": \\\"Admin\\\"\\n      }\\n    },\\n    \\\"waitForSuccess\\\": true\\n  }\\n}\\nEOF\\n\\n# Create the GPU pool\\naz batch pool create \\\\\\n  --json-file hpc-gpu-pool-config.json\\n\\n# Create memory-intensive compute pool (M series for large datasets)\\necho \\\"Creating memory-intensive compute pool...\\\"\\ncat > hpc-memory-pool-config.json << 'EOF'\\n{\\n  \\\"id\\\": \\\"hpc-memory-pool\\\",\\n  \\\"displayName\\\": \\\"HPC Memory Pool - M series instances\\\",\\n  \\\"vmSize\\\": \\\"Standard_M64s\\\",\\n  \\\"virtualMachineConfiguration\\\": {\\n    \\\"imageReference\\\": {\\n      \\\"publisher\\\": \\\"Canonical\\\",\\n      \\\"offer\\\": \\\"0001-com-ubuntu-server-focal\\\",\\n      \\\"sku\\\": \\\"20_04-lts-gen2\\\",\\n      \\\"version\\\": \\\"latest\\\"\\n    },\\n    \\\"nodeAgentSkuId\\\": \\\"batch.node.ubuntu 20.04\\\"\\n  },\\n  \\\"targetDedicatedNodes\\\": 0,\\n  \\\"targetLowPriorityNodes\\\": 2,\\n  \\\"enableAutoScale\\\": true,\\n  \\\"autoScaleFormula\\\": \\\"// Memory-intensive workload scaling\\\\n$samples = $ActiveTasks.GetSamplePercent(TimeInterval_Minute * 20);\\\\n$tasks = $samples < 70 ? max(0, $ActiveTasks.GetSample(1)) : max(0, avg($ActiveTasks.GetSample(TimeInterval_Minute * 20)));\\\\n$memoryTasks = $tasks; // Memory workloads typically need fewer but larger nodes\\\\n$TargetDedicated = max(0, min(1, $memoryTasks));\\\\n$TargetLowPriority = max(0, min(4, $tasks - $TargetDedicated));\\\",\\n  \\\"autoScaleEvaluationInterval\\\": \\\"PT10M\\\",\\n  \\\"enableInterNodeCommunication\\\": false,\\n  \\\"maxTasksPerNode\\\": 4,\\n  \\\"taskSlotsPerNode\\\": 4,\\n  \\\"startTask\\\": {\\n    \\\"commandLine\\\": \\\"/bin/bash -c 'free -h && df -h && lscpu'\\\",\\n    \\\"userIdentity\\\": {\\n      \\\"autoUser\\\": {\\n        \\\"scope\\\": \\\"Pool\\\",\\n        \\\"elevationLevel\\\": \\\"NonAdmin\\\"\\n      }\\n    },\\n    \\\"waitForSuccess\\\": true\\n  }\\n}\\nEOF\\n\\n# Create the memory pool\\naz batch pool create \\\\\\n  --json-file hpc-memory-pool-config.json\\n\\necho \\\"HPC compute pools created successfully.\\\"\\necho \\\"\\nPools created:\\\"\\naz batch pool list --query '[].{Id:id, VmSize:vmSize, State:state}' --output table\"\n        },\n        \"setup_hpc_applications\": {\n          \"language\": \"bash\",\n          \"title\": \"Setup HPC Applications and Workflows\",\n          \"code\": \"# Setup HPC applications and sample workflows\\n\\necho \\\"Setting up HPC applications and workflows...\\\"\\n\\n# Create application packages for common HPC software\\necho \\\"Creating application packages...\\\"\\n\\n# Upload OpenMPI application package\\necho \\\"Preparing OpenMPI application package...\\\"\\nmkdir -p /tmp/hpc-apps/openmpi\\ncat > /tmp/hpc-apps/openmpi/install-openmpi.sh << 'EOF'\\n#!/bin/bash\\n# OpenMPI installation script for HPC nodes\\n\\nset -e\\n\\n# Install prerequisites\\nsudo apt-get update\\nsudo apt-get install -y build-essential gfortran wget\\n\\n# Download and install OpenMPI\\ncd /tmp\\nwget https://download.open-mpi.org/release/open-mpi/v4.1/openmpi-4.1.4.tar.gz\\ntar -xzf openmpi-4.1.4.tar.gz\\ncd openmpi-4.1.4\\n\\n./configure --prefix=/opt/openmpi --enable-mpirun-prefix-by-default\\nmake -j$(nproc)\\nsudo make install\\n\\n# Set environment variables\\necho 'export PATH=/opt/openmpi/bin:$PATH' | sudo tee -a /etc/environment\\necho 'export LD_LIBRARY_PATH=/opt/openmpi/lib:$LD_LIBRARY_PATH' | sudo tee -a /etc/environment\\n\\n# Verify installation\\n/opt/openmpi/bin/mpirun --version\\nEOF\\n\\nchmod +x /tmp/hpc-apps/openmpi/install-openmpi.sh\\n\\n# Create MPI Hello World application\\ncat > /tmp/hpc-apps/openmpi/hello_mpi.c << 'EOF'\\n#include <mpi.h>\\n#include <stdio.h>\\n#include <unistd.h>\\n\\nint main(int argc, char** argv) {\\n    MPI_Init(NULL, NULL);\\n\\n    int world_size;\\n    MPI_Comm_size(MPI_COMM_WORLD, &world_size);\\n\\n    int world_rank;\\n    MPI_Comm_rank(MPI_COMM_WORLD, &world_rank);\\n\\n    char processor_name[MPI_MAX_PROCESSOR_NAME];\\n    int name_len;\\n    MPI_Get_processor_name(processor_name, &name_len);\\n\\n    printf(\\\"Hello from processor %s, rank %d out of %d processors\\\\n\\\",\\n           processor_name, world_rank, world_size);\\n\\n    // Simulate some work\\n    sleep(5);\\n\\n    MPI_Finalize();\\n    return 0;\\n}\\nEOF\\n\\n# Create compilation script\\ncat > /tmp/hpc-apps/openmpi/compile-hello.sh << 'EOF'\\n#!/bin/bash\\n# Compile MPI Hello World\\nsource /etc/environment\\n/opt/openmpi/bin/mpicc -o hello_mpi hello_mpi.c\\nEOF\\n\\nchmod +x /tmp/hpc-apps/openmpi/compile-hello.sh\\n\\n# Create application package archive\\ncd /tmp/hpc-apps\\ntar -czf openmpi-package.tar.gz openmpi/\\n\\n# Upload application package to Batch\\necho \\\"Uploading OpenMPI application package...\\\"\\naz batch application create \\\\\\n  --application-id openmpi \\\\\\n  --display-name \\\"OpenMPI 4.1.4\\\"\\n\\naz batch application package create \\\\\\n  --application-id openmpi \\\\\\n  --package-file /tmp/hpc-apps/openmpi-package.tar.gz \\\\\\n  --version 4.1.4\\n\\n# Create Python scientific computing package\\necho \\\"Creating Python scientific package...\\\"\\nmkdir -p /tmp/hpc-apps/python-sci\\n\\ncat > /tmp/hpc-apps/python-sci/install-python-sci.sh << 'EOF'\\n#!/bin/bash\\n# Install Python scientific computing stack\\n\\nset -e\\n\\n# Update system\\nsudo apt-get update\\nsudo apt-get install -y python3 python3-pip python3-dev\\n\\n# Install scientific computing packages\\npip3 install --user numpy scipy matplotlib pandas scikit-learn\\npip3 install --user mpi4py  # MPI for Python\\npip3 install --user numba   # JIT compilation\\npip3 install --user dask[complete]  # Parallel computing\\n\\n# Verify installation\\npython3 -c \\\"import numpy, scipy, matplotlib, pandas, sklearn, mpi4py, numba, dask; print('All packages installed successfully')\\\"\\nEOF\\n\\nchmod +x /tmp/hpc-apps/python-sci/install-python-sci.sh\\n\\n# Create sample Python HPC application\\ncat > /tmp/hpc-apps/python-sci/monte_carlo_pi.py << 'EOF'\\n#!/usr/bin/env python3\\n\\\"\\\"\\\"\\nMonte Carlo Pi estimation using MPI\\n\\\"\\\"\\\"\\n\\nimport numpy as np\\nfrom mpi4py import MPI\\nimport time\\n\\ndef estimate_pi(n_samples):\\n    \\\"\\\"\\\"Estimate Pi using Monte Carlo method\\\"\\\"\\\"\\n    # Generate random points\\n    x = np.random.uniform(-1, 1, n_samples)\\n    y = np.random.uniform(-1, 1, n_samples)\\n    \\n    # Check if points are inside unit circle\\n    inside_circle = (x**2 + y**2) <= 1\\n    \\n    # Estimate Pi\\n    pi_estimate = 4 * np.sum(inside_circle) / n_samples\\n    return pi_estimate, np.sum(inside_circle)\\n\\ndef main():\\n    comm = MPI.COMM_WORLD\\n    rank = comm.Get_rank()\\n    size = comm.Get_size()\\n    \\n    # Total number of samples (will be distributed across processes)\\n    total_samples = 10000000\\n    samples_per_process = total_samples // size\\n    \\n    if rank == 0:\\n        print(f\\\"Estimating Pi using {total_samples} samples across {size} processes\\\")\\n        start_time = time.time()\\n    \\n    # Each process estimates Pi with its portion of samples\\n    local_pi, local_inside = estimate_pi(samples_per_process)\\n    \\n    # Gather results from all processes\\n    all_inside = comm.gather(local_inside, root=0)\\n    \\n    if rank == 0:\\n        # Calculate final Pi estimate\\n        total_inside = sum(all_inside)\\n        pi_final = 4 * total_inside / total_samples\\n        \\n        end_time = time.time()\\n        \\n        print(f\\\"Pi estimate: {pi_final}\\\")\\n        print(f\\\"Actual Pi:   {np.pi}\\\")\\n        print(f\\\"Error:       {abs(pi_final - np.pi):.6f}\\\")\\n        print(f\\\"Runtime:     {end_time - start_time:.2f} seconds\\\")\\n        print(f\\\"Samples inside circle: {total_inside} out of {total_samples}\\\")\\n\\nif __name__ == \\\"__main__\\\":\\n    main()\\nEOF\\n\\nchmod +x /tmp/hpc-apps/python-sci/monte_carlo_pi.py\\n\\n# Create Python package archive\\ncd /tmp/hpc-apps\\ntar -czf python-sci-package.tar.gz python-sci/\\n\\n# Upload Python application package\\necho \\\"Uploading Python scientific package...\\\"\\naz batch application create \\\\\\n  --application-id python-sci \\\\\\n  --display-name \\\"Python Scientific Computing Stack\\\"\\n\\naz batch application package create \\\\\\n  --application-id python-sci \\\\\\n  --package-file /tmp/hpc-apps/python-sci-package.tar.gz \\\\\\n  --version 1.0\\n\\necho \\\"HPC applications setup completed successfully.\\\"\\necho \\\"\\nApplication packages created:\\\"\\naz batch application list --query '[].{Id:id, DisplayName:displayName}' --output table\"\n        }\n      }\n    },\n    {\n      \"title\": \"High-Performance Storage and Data Management\",\n      \"content\": \"Implement advanced storage architectures for HPC workloads equivalent to AWS FSx for Lustre and high-performance storage patterns.\",\n      \"code_examples\": {\n        \"configure_hpc_cache_storage\": {\n          \"language\": \"bash\",\n          \"title\": \"Configure HPC Cache with Storage Targets\",\n          \"code\": \"# Configure HPC Cache with multiple storage targets\\n\\necho \\\"Configuring HPC Cache with storage targets...\\\"\\n\\n# Wait for HPC Cache to be fully provisioned\\necho \\\"Waiting for HPC Cache provisioning...\\\"\\naz hpc-cache wait \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --name $HPC_CACHE_NAME \\\\\\n  --created\\n\\n# Create additional storage account for archive data\\necho \\\"Creating archive storage account...\\\"\\nARCHIVE_STORAGE_NAME=\\\"hpcarchive$(openssl rand -hex 3)\\\"\\naz storage account create \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --name $ARCHIVE_STORAGE_NAME \\\\\\n  --location eastus \\\\\\n  --sku Standard_LRS \\\\\\n  --kind StorageV2 \\\\\\n  --access-tier Hot\\n\\n# Create blob container for archive data\\naz storage container create \\\\\\n  --account-name $ARCHIVE_STORAGE_NAME \\\\\\n  --name hpc-results \\\\\\n  --auth-mode login\\n\\naz storage container create \\\\\\n  --account-name $ARCHIVE_STORAGE_NAME \\\\\\n  --name hpc-datasets \\\\\\n  --auth-mode login\\n\\n# Add Azure Blob storage target to HPC Cache\\necho \\\"Adding Blob storage target to HPC Cache...\\\"\\naz hpc-cache blob-storage-target add \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --cache-name $HPC_CACHE_NAME \\\\\\n  --name blob-target-results \\\\\\n  --storage-account $ARCHIVE_STORAGE_NAME \\\\\\n  --container-name hpc-results \\\\\\n  --virtual-namespace-path /results\\n\\naz hpc-cache blob-storage-target add \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --cache-name $HPC_CACHE_NAME \\\\\\n  --name blob-target-datasets \\\\\\n  --storage-account $ARCHIVE_STORAGE_NAME \\\\\\n  --container-name hpc-datasets \\\\\\n  --virtual-namespace-path /datasets\\n\\n# Add Azure Files storage target for shared file system\\necho \\\"Adding Azure Files storage target to HPC Cache...\\\"\\naz hpc-cache nfs-storage-target add \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --cache-name $HPC_CACHE_NAME \\\\\\n  --name nfs-target-shared \\\\\\n  --nfs3-target $HPC_STORAGE_NAME.file.core.windows.net \\\\\\n  --nfs3-usage-model WRITE_WORKLOAD_15 \\\\\\n  --junction namespace-path=/shared target-path=/hpc-shared\\n\\n# Create NetApp Files account for high-performance NFS (optional)\\necho \\\"Note: NetApp Files provides the highest performance equivalent to FSx for Lustre\\\"\\necho \\\"Creating NetApp Files configuration (requires manual setup in portal)...\\\"\\n\\n# Create configuration template for NetApp Files\\ncat > netapp-files-config.json << 'EOF'\\n{\\n  \\\"netAppAccount\\\": {\\n    \\\"name\\\": \\\"netapp-hpc-account\\\",\\n    \\\"location\\\": \\\"eastus\\\",\\n    \\\"tags\\\": {\\n      \\\"Environment\\\": \\\"HPC\\\",\\n      \\\"Performance\\\": \\\"Ultra\\\"\\n    }\\n  },\\n  \\\"capacityPools\\\": [\\n    {\\n      \\\"name\\\": \\\"hpc-ultra-pool\\\",\\n      \\\"serviceLevel\\\": \\\"Ultra\\\",\\n      \\\"size\\\": \\\"4TB\\\",\\n      \\\"qosType\\\": \\\"Auto\\\"\\n    }\\n  ],\\n  \\\"volumes\\\": [\\n    {\\n      \\\"name\\\": \\\"hpc-scratch-volume\\\",\\n      \\\"capacityPool\\\": \\\"hpc-ultra-pool\\\",\\n      \\\"size\\\": \\\"2TB\\\",\\n      \\\"serviceLevel\\\": \\\"Ultra\\\",\\n      \\\"throughputMibps\\\": 2048,\\n      \\\"exportPolicy\\\": {\\n        \\\"rules\\\": [\\n          {\\n            \\\"ruleIndex\\\": 1,\\n            \\\"allowedClients\\\": \\\"10.20.0.0/16\\\",\\n            \\\"protocols\\\": [\\\"NFSv3\\\"],\\n            \\\"accessType\\\": \\\"ReadWrite\\\",\\n            \\\"rootAccess\\\": true\\n          }\\n        ]\\n      }\\n    }\\n  ]\\n}\\nEOF\\n\\n# Create data lifecycle management policy\\necho \\\"Creating data lifecycle management policy...\\\"\\ncat > data-lifecycle-policy.json << 'EOF'\\n{\\n  \\\"rules\\\": [\\n    {\\n      \\\"name\\\": \\\"MoveToArchive\\\",\\n      \\\"enabled\\\": true,\\n      \\\"type\\\": \\\"Lifecycle\\\",\\n      \\\"definition\\\": {\\n        \\\"filters\\\": {\\n          \\\"blobTypes\\\": [\\\"blockBlob\\\"],\\n          \\\"prefixMatch\\\": [\\\"results/\\\"]\\n        },\\n        \\\"actions\\\": {\\n          \\\"baseBlob\\\": {\\n            \\\"tierToCool\\\": {\\n              \\\"daysAfterModificationGreaterThan\\\": 30\\n            },\\n            \\\"tierToArchive\\\": {\\n              \\\"daysAfterModificationGreaterThan\\\": 90\\n            },\\n            \\\"delete\\\": {\\n              \\\"daysAfterModificationGreaterThan\\\": 2555\\n            }\\n          }\\n        }\\n      }\\n    },\\n    {\\n      \\\"name\\\": \\\"DeleteTempData\\\",\\n      \\\"enabled\\\": true,\\n      \\\"type\\\": \\\"Lifecycle\\\",\\n      \\\"definition\\\": {\\n        \\\"filters\\\": {\\n          \\\"blobTypes\\\": [\\\"blockBlob\\\"],\\n          \\\"prefixMatch\\\": [\\\"temp/\\\", \\\"scratch/\\\"]\\n        },\\n        \\\"actions\\\": {\\n          \\\"baseBlob\\\": {\\n            \\\"delete\\\": {\\n              \\\"daysAfterModificationGreaterThan\\\": 7\\n            }\\n          }\\n        }\\n      }\\n    }\\n  ]\\n}\\nEOF\\n\\n# Apply lifecycle policy to archive storage\\naz storage account management-policy create \\\\\\n  --account-name $ARCHIVE_STORAGE_NAME \\\\\\n  --policy @data-lifecycle-policy.json\\n\\necho \\\"HPC storage configuration completed successfully.\\\"\\necho \\\"\\nHPC Cache storage targets:\\\"\\naz hpc-cache storage-target list \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --cache-name $HPC_CACHE_NAME \\\\\\n  --query '[].{Name:name, Type:targetType, State:state}' --output table\"\n        },\n        \"setup_hpc_monitoring\": {\n          \"language\": \"bash\",\n          \"title\": \"Setup Advanced HPC Monitoring and Performance Analysis\",\n          \"code\": \"# Setup comprehensive HPC monitoring and performance analysis\\n\\necho \\\"Setting up HPC monitoring and performance analysis...\\\"\\n\\n# Install VM extensions for monitoring on compute nodes\\necho \\\"Note: In production, these extensions would be installed via VM scale sets or batch pools\\\"\\n\\n# Create custom metrics and alerts for HPC workloads\\necho \\\"Creating HPC-specific monitoring alerts...\\\"\\n\\n# CPU utilization alert for HPC nodes\\naz monitor metrics alert create \\\\\\n  --name \\\"HPC-High-CPU-Utilization\\\" \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --scopes $(az batch account show --resource-group hpc-cluster-rg --name $BATCH_ACCOUNT_NAME --query id -o tsv) \\\\\\n  --condition \\\"avg Percentage CPU > 90\\\" \\\\\\n  --window-size 5m \\\\\\n  --evaluation-frequency 1m \\\\\\n  --severity 2 \\\\\\n  --description \\\"HPC nodes experiencing high CPU utilization\\\"\\n\\n# Memory utilization alert\\naz monitor metrics alert create \\\\\\n  --name \\\"HPC-High-Memory-Utilization\\\" \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --scopes $(az batch account show --resource-group hpc-cluster-rg --name $BATCH_ACCOUNT_NAME --query id -o tsv) \\\\\\n  --condition \\\"avg Available Memory Bytes < 1073741824\\\" \\\\\\n  --window-size 5m \\\\\\n  --evaluation-frequency 1m \\\\\\n  --severity 1 \\\\\\n  --description \\\"HPC nodes running low on available memory\\\"\\n\\n# HPC Cache performance alert\\naz monitor metrics alert create \\\\\\n  --name \\\"HPC-Cache-High-Latency\\\" \\\\\\n  --resource-group hpc-cluster-rg \\\\\\n  --scopes $(az hpc-cache show --resource-group hpc-cluster-rg --name $HPC_CACHE_NAME --query id -o tsv) \\\\\\n  --condition \\\"avg ClientLatency > 10\\\" \\\\\\n  --window-size 5m \\\\\\n  --evaluation-frequency 1m \\\\\\n  --severity 2 \\\\\\n  --description \\\"HPC Cache experiencing high client latency\\\"\\n\\n# Create Log Analytics queries for HPC performance analysis\\necho \\\"Creating Log Analytics queries for HPC analysis...\\\"\\n\\ncat > hpc-performance-queries.kql << 'EOF'\\n// HPC Performance Analysis Queries for Log Analytics\\n\\n// 1. Top CPU consuming processes across HPC nodes\\nPerf\\n| where ObjectName == \\\"Process\\\" and CounterName == \\\"% Processor Time\\\"\\n| where TimeGenerated > ago(1h)\\n| summarize AvgCpuUsage = avg(CounterValue) by Computer, InstanceName\\n| where AvgCpuUsage > 50\\n| order by AvgCpuUsage desc\\n| take 20\\n\\n// 2. Memory usage trends for HPC workloads\\nPerf\\n| where ObjectName == \\\"Memory\\\" and CounterName == \\\"Available MBytes\\\"\\n| where TimeGenerated > ago(4h)\\n| summarize AvailableMemory = avg(CounterValue) by Computer, bin(TimeGenerated, 15m)\\n| render timechart\\n\\n// 3. Network I/O analysis for MPI communications\\nPerf\\n| where ObjectName == \\\"Network Interface\\\" \\n| where CounterName in (\\\"Bytes Received/sec\\\", \\\"Bytes Sent/sec\\\")\\n| where TimeGenerated > ago(2h)\\n| summarize NetworkThroughput = avg(CounterValue) by Computer, CounterName, bin(TimeGenerated, 5m)\\n| render timechart\\n\\n// 4. Disk I/O performance for storage-intensive workloads\\nPerf\\n| where ObjectName == \\\"LogicalDisk\\\" and CounterName in (\\\"Disk Reads/sec\\\", \\\"Disk Writes/sec\\\", \\\"Avg. Disk sec/Read\\\", \\\"Avg. Disk sec/Write\\\")\\n| where TimeGenerated > ago(2h)\\n| summarize DiskPerformance = avg(CounterValue) by Computer, CounterName, bin(TimeGenerated, 10m)\\n| render timechart\\n\\n// 5. HPC job completion and failure analysis\\nBatchNodeEvent\\n| where TimeGenerated > ago(1d)\\n| summarize Count = count() by EventType, PoolId, NodeId\\n| order by Count desc\\n\\n// 6. Cost analysis for HPC workloads\\nBatchPoolEvent\\n| where TimeGenerated > ago(7d)\\n| extend CostPerHour = case(\\n    VmSize contains \\\"HB\\\", 3.60,  // Approximate cost per hour for HBv3\\n    VmSize contains \\\"ND\\\", 24.48, // Approximate cost per hour for NDv2\\n    VmSize contains \\\"NC\\\", 3.42,  // Approximate cost per hour for NCv3\\n    1.50 // Default cost for other VM sizes\\n)\\n| summarize TotalCost = sum(CostPerHour), TotalHours = count() by PoolId, VmSize\\n| project PoolId, VmSize, TotalHours, TotalCost, AvgCostPerHour = TotalCost / TotalHours\\n| order by TotalCost desc\\nEOF\\n\\n# Create performance monitoring dashboard (JSON template)\\necho \\\"Creating HPC performance dashboard template...\\\"\\ncat > hpc-dashboard.json << 'EOF'\\n{\\n  \\\"lenses\\\": {\\n    \\\"0\\\": {\\n      \\\"order\\\": 0,\\n      \\\"parts\\\": {\\n        \\\"0\\\": {\\n          \\\"position\\\": { \\\"x\\\": 0, \\\"y\\\": 0, \\\"colSpan\\\": 6, \\\"rowSpan\\\": 4 },\\n          \\\"metadata\\\": {\\n            \\\"inputs\\\": [{\\n              \\\"name\\\": \\\"resourceTypeMode\\\",\\n              \\\"isOptional\\\": true\\n            }],\\n            \\\"type\\\": \\\"Extension/HubsExtension/PartType/MonitorChartPart\\\",\\n            \\\"settings\\\": {\\n              \\\"content\\\": {\\n                \\\"options\\\": {\\n                  \\\"chart\\\": {\\n                    \\\"metrics\\\": [{\\n                      \\\"resourceMetadata\\\": {\\n                        \\\"id\\\": \\\"/subscriptions/SUBSCRIPTION_ID/resourceGroups/hpc-cluster-rg/providers/Microsoft.Batch/batchAccounts/BATCH_ACCOUNT_NAME\\\"\\n                      },\\n                      \\\"name\\\": \\\"RunningNodeCount\\\",\\n                      \\\"aggregationType\\\": 4,\\n                      \\\"metricVisualization\\\": {\\n                        \\\"displayName\\\": \\\"Running Node Count\\\"\\n                      }\\n                    }],\\n                    \\\"title\\\": \\\"HPC Cluster - Running Nodes\\\",\\n                    \\\"titleKind\\\": 1,\\n                    \\\"visualization\\\": {\\n                      \\\"chartType\\\": 2\\n                    }\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        },\\n        \\\"1\\\": {\\n          \\\"position\\\": { \\\"x\\\": 6, \\\"y\\\": 0, \\\"colSpan\\\": 6, \\\"rowSpan\\\": 4 },\\n          \\\"metadata\\\": {\\n            \\\"inputs\\\": [{\\n              \\\"name\\\": \\\"resourceTypeMode\\\",\\n              \\\"isOptional\\\": true\\n            }],\\n            \\\"type\\\": \\\"Extension/HubsExtension/PartType/MonitorChartPart\\\",\\n            \\\"settings\\\": {\\n              \\\"content\\\": {\\n                \\\"options\\\": {\\n                  \\\"chart\\\": {\\n                    \\\"metrics\\\": [{\\n                      \\\"resourceMetadata\\\": {\\n                        \\\"id\\\": \\\"/subscriptions/SUBSCRIPTION_ID/resourceGroups/hpc-cluster-rg/providers/Microsoft.StorageCache/caches/HPC_CACHE_NAME\\\"\\n                      },\\n                      \\\"name\\\": \\\"ClientIOPS\\\",\\n                      \\\"aggregationType\\\": 4,\\n                      \\\"metricVisualization\\\": {\\n                        \\\"displayName\\\": \\\"Client IOPS\\\"\\n                      }\\n                    }],\\n                    \\\"title\\\": \\\"HPC Cache - Client IOPS\\\",\\n                    \\\"titleKind\\\": 1,\\n                    \\\"visualization\\\": {\\n                      \\\"chartType\\\": 2\\n                    }\\n                  }\\n                }\\n              }\\n            }\\n          }\\n        }\\n      }\\n    }\\n  },\\n  \\\"metadata\\\": {\\n    \\\"model\\\": {\\n      \\\"timeRange\\\": {\\n        \\\"value\\\": {\\n          \\\"relative\\\": {\\n            \\\"duration\\\": 24,\\n            \\\"timeUnit\\\": 1\\n          }\\n        },\\n        \\\"type\\\": \\\"MsPortalFx.Composition.Configuration.ValueTypes.TimeRange\\\"\\n      }\\n    }\\n  }\\n}\\nEOF\\n\\n# Replace placeholders\\nsed -i \\\"s/SUBSCRIPTION_ID/$(az account show --query id -o tsv)/g\\\" hpc-dashboard.json\\nsed -i \\\"s/BATCH_ACCOUNT_NAME/$BATCH_ACCOUNT_NAME/g\\\" hpc-dashboard.json\\nsed -i \\\"s/HPC_CACHE_NAME/$HPC_CACHE_NAME/g\\\" hpc-dashboard.json\\n\\necho \\\"HPC monitoring setup completed successfully.\\\"\\necho \\\"\\nMonitoring alerts created:\\\"\\naz monitor metrics alert list --resource-group hpc-cluster-rg --query '[].{Name:name, Condition:criteria.allOf[0].criterionType, Severity:severity}' --output table\"\n        }\n      }\n    }\n  ],\n  \"hands_on_exercise\": {\n    \"scenario\": \"Deploy and optimize enterprise HPC environment for scientific computing workloads\",\n    \"requirements\": [\n      \"Create comprehensive HPC infrastructure with multiple compute pools\",\n      \"Configure high-performance storage with HPC Cache and tiered storage\",\n      \"Deploy HPC applications including OpenMPI and Python scientific stack\",\n      \"Set up advanced monitoring and performance analysis dashboards\",\n      \"Implement cost optimization with auto-scaling and spot instances\",\n      \"Configure data lifecycle management for HPC datasets and results\",\n      \"Create HPC job submission and management workflows\",\n      \"Establish HPC security and compliance frameworks\"\n    ],\n    \"validation_steps\": [\n      {\n        \"step\": \"Verify HPC Cache deployment\",\n        \"command\": \"az hpc-cache show --resource-group hpc-cluster-rg --name $HPC_CACHE_NAME --query 'provisioningState'\",\n        \"expected\": \"Should return 'Succeeded'\"\n      },\n      {\n        \"step\": \"Check Batch compute pools\",\n        \"command\": \"az batch pool list --query 'length(@)'\",\n        \"expected\": \"Should return count of compute pools (minimum 3)\"\n      },\n      {\n        \"step\": \"Verify HPC applications\",\n        \"command\": \"az batch application list --query 'length(@)'\",\n        \"expected\": \"Should return count of application packages (minimum 2)\"\n      },\n      {\n        \"step\": \"Check storage targets\",\n        \"command\": \"az hpc-cache storage-target list --resource-group hpc-cluster-rg --cache-name $HPC_CACHE_NAME --query 'length(@)'\",\n        \"expected\": \"Should return count of storage targets (minimum 3)\"\n      },\n      {\n        \"step\": \"Verify monitoring alerts\",\n        \"command\": \"az monitor metrics alert list --resource-group hpc-cluster-rg --query 'length(@)'\",\n        \"expected\": \"Should return count of monitoring alerts (minimum 3)\"\n      }\n    ]\n  },\n  \"hpc_workload_optimization\": {\n    \"compute_optimization\": {\n      \"vm_selection\": {\n        \"cpu_intensive\": \"HBv3 series for tightly coupled MPI workloads\",\n        \"memory_intensive\": \"M series for large in-memory datasets\",\n        \"gpu_computing\": \"NDv2 series for AI/ML and CUDA workloads\",\n        \"general_purpose\": \"Dv3 series for loosely coupled workloads\"\n      },\n      \"scaling_strategies\": {\n        \"auto_scaling\": \"Dynamic scaling based on queue depth and resource utilization\",\n        \"spot_instances\": \"Cost optimization with Azure Spot VMs for fault-tolerant workloads\",\n        \"dedicated_hosts\": \"Regulatory compliance with dedicated hardware\"\n      }\n    },\n    \"storage_optimization\": {\n      \"performance_tiers\": {\n        \"ultra_ssd\": \"Highest IOPS for scratch storage and temporary data\",\n        \"premium_ssd\": \"High performance for application data and databases\",\n        \"standard_ssd\": \"Balanced performance for general workloads\",\n        \"archive_storage\": \"Long-term retention with lifecycle policies\"\n      },\n      \"data_movement\": {\n        \"data_box\": \"Large-scale data migration for initial HPC dataset transfer\",\n        \"expressroute\": \"High-bandwidth connectivity for ongoing data synchronization\",\n        \"azcopy\": \"Optimized data transfer utilities with parallelization\"\n      }\n    },\n    \"network_optimization\": {\n      \"infiniband\": \"Low-latency networking for tightly coupled MPI applications\",\n      \"accelerated_networking\": \"SR-IOV for improved network performance\",\n      \"proximity_placement\": \"Physical co-location for latency-sensitive workloads\"\n    }\n  },\n  \"cost_management_strategies\": {\n    \"compute_costs\": {\n      \"spot_instances\": \"Up to 90% cost savings for fault-tolerant workloads\",\n      \"reserved_instances\": \"Significant discounts for predictable workloads\",\n      \"auto_scaling\": \"Pay only for resources when actively computing\",\n      \"hybrid_benefit\": \"Use existing licenses for Windows Server and SQL Server\"\n    },\n    \"storage_costs\": {\n      \"tiered_storage\": \"Automated data movement based on access patterns\",\n      \"lifecycle_policies\": \"Automatic archival and deletion of old data\",\n      \"deduplication\": \"Reduce storage requirements for similar datasets\"\n    },\n    \"monitoring_costs\": {\n      \"cost_budgets\": \"Proactive alerts for unexpected spending\",\n      \"resource_tagging\": \"Detailed cost allocation by project and department\",\n      \"advisor_recommendations\": \"Automated optimization suggestions\"\n    }\n  },\n  \"cleanup\": {\n    \"instructions\": \"Remove HPC resources in proper order to avoid dependencies\",\n    \"commands\": [\n      \"# Delete Batch pools (this will stop all running tasks)\",\n      \"az batch pool delete --pool-id hpc-cpu-pool --yes\",\n      \"az batch pool delete --pool-id hpc-gpu-pool --yes\",\n      \"az batch pool delete --pool-id hpc-memory-pool --yes\",\n      \"# Delete application packages\",\n      \"az batch application delete --application-id openmpi --yes\",\n      \"az batch application delete --application-id python-sci --yes\",\n      \"# Delete HPC Cache (this may take several minutes)\",\n      \"az hpc-cache delete --resource-group hpc-cluster-rg --name $HPC_CACHE_NAME --yes --no-wait\",\n      \"# Delete Batch account\",\n      \"az batch account delete --resource-group hpc-cluster-rg --name $BATCH_ACCOUNT_NAME --yes\",\n      \"# Delete monitoring alerts\",\n      \"az monitor metrics alert delete --resource-group hpc-cluster-rg --name HPC-High-CPU-Utilization\",\n      \"az monitor metrics alert delete --resource-group hpc-cluster-rg --name HPC-High-Memory-Utilization\",\n      \"az monitor metrics alert delete --resource-group hpc-cluster-rg --name HPC-Cache-High-Latency\",\n      \"# Delete storage accounts\",\n      \"az storage account delete --resource-group hpc-cluster-rg --name $HPC_STORAGE_NAME --yes\",\n      \"az storage account delete --resource-group hpc-cluster-rg --name $ARCHIVE_STORAGE_NAME --yes\",\n      \"# Delete Key Vault\",\n      \"az keyvault delete --resource-group hpc-cluster-rg --name $HPC_KEYVAULT_NAME\",\n      \"az keyvault purge --name $HPC_KEYVAULT_NAME\",\n      \"# Delete Log Analytics workspace\",\n      \"az monitor log-analytics workspace delete --resource-group hpc-cluster-rg --workspace-name hpc-monitoring-workspace --yes\",\n      \"# Delete resource group\",\n      \"az group delete --name hpc-cluster-rg --yes --no-wait\"\n    ]\n  },\n  \"key_takeaways\": [\n    \"Azure HPC provides more integrated services than AWS for high-performance computing\",\n    \"HPC Cache offers superior performance compared to AWS FSx for Lustre in many scenarios\",\n    \"Azure Batch provides more sophisticated auto-scaling capabilities than AWS Batch\",\n    \"CycleCloud offers better cluster management and orchestration than AWS ParallelCluster\",\n    \"InfiniBand networking in Azure provides lower latency than AWS placement groups\",\n    \"Cost optimization options are more granular with Azure Spot VMs and scaling policies\",\n    \"Integration with Azure AI/ML services provides better workflows for HPC+AI workloads\"\n  ],\n  \"next_steps\": [\n    \"Learn Azure Machine Learning for HPC+AI integrated workloads\",\n    \"Explore Azure Quantum for quantum computing integration\",\n    \"Study Azure Digital Twins for simulation and modeling workloads\",\n    \"Practice with Azure Arc for hybrid HPC cluster management\",\n    \"Learn about Azure confidential computing for sensitive HPC workloads\"\n  ]\n}"
        }
      }
    }
  ]
}